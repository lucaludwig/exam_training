[
  {
    "question": "A company plans to deploy an ML model for production inference on an Amazon SageMaker endpoint. The average inference payload size will vary from 100 MB to 300 MB. Inference requests must be processed in 60 minutes or less.\nWhich SageMaker inference option will meet these requirements?",
    "options": [
      "A. Serverless inference",
      "B. Asynchronous inference",
      "C. Real-time inference",
      "D. Batch transform"
    ],
    "answer": "B"
  },
  {
    "question": "An ML engineer needs to encrypt all data in transit when an ML training job runs. The ML engineer must ensure that encryption in transit is applied to processes that Amazon SageMaker uses during the training job.\nWhich solution will meet these requirements?",
    "options": [
      "A. Encrypt communication between nodes for batch processing.",
      "B. Encrypt communication between nodes in a training cluster.",
      "C. Specify an AWS Key Management Service (AWS KMS) key during creation of the training job request.",
      "D. Specify an AWS Key Management Service (AWS KMS) key during creation of the SageMaker domain."
    ],
    "answer": "B"
  },
  {
    "question": "An ML engineer is developing a fraud detection model by using the Amazon SageMaker XGBoost algorithm. The model classifies transactions as either fraudulent or legitimate. During testing, the model excels at identifying fraud in the training dataset. However, the model is inefficient at identifying fraud in new and unseen transactions.\nWhat should the ML engineer do to improve the fraud detection for new transactions?",
    "options": [
      "A. Increase the learning rate.",
      "B. Remove some irrelevant features from the training dataset.",
      "C. Increase the value of the max_depth hyperparameter.",
      "D. Decrease the value of the max_depth hyperparameter."
    ],
    "answer": "D"
  },
  {
    "question": "A company is using ML to predict the presence of a specific weed in a farmer's field. The company is using the Amazon SageMaker linear learner built-in algorithm with a value of multiclass_classifier for the predictor_type hyperparameter.\nWhat should the company do to MINIMIZE false positives?",
    "options": [
      "A. Set the value of the weight decay hyperparameter to zero.",
      "B. Increase the number of training epochs.",
      "C. Increase the value of the target_precision hyperparameter.",
      "D. Change the value of the predictor_type hyperparameter to regressor."
    ],
    "answer": "C"
  },
  {
    "question": "A company has an ML model that needs to run one time each night to predict stock values. The model input is 3 MB of data that is collected during the current day. The model produces the predictions for the next day. The prediction process takes less than 1 minute to finish running.\nHow should the company deploy the model on Amazon SageMaker to meet these requirements?",
    "options": [
      "A. Use a multi-model serverless endpoint. Enable caching.",
      "B. Use an asynchronous inference endpoint. Set the InitialInstanceCount parameter to 0.",
      "C. Use a real-time endpoint. Configure an auto scaling policy to scale the model to 0 when the model is not in use.",
      "D. Use a serverless inference endpoint. Set the MaxConcurrency parameter to 1."
    ],
    "answer": "D"
  },
  {
    "question": "An ML engineer has developed a binary classification model outside of Amazon SageMaker. The ML engineer needs to make the model accessible to a SageMaker Canvas user for additional tuning.\nThe model artifacts are stored in an Amazon S3 bucket. The ML engineer and the Canvas user are part of the same SageMaker domain.\nWhich combination of requirements must be met so that the ML engineer can share the model with the Canvas user? (Choose two.)",
    "options": [
      "A. The ML engineer and the Canvas user must be in separate SageMaker domains.",
      "B. The Canvas user must have permissions to access the S3 bucket where the model artifacts are stored.",
      "C. The model must be registered in the SageMaker Model Registry.",
      "D. The ML engineer must host the model on AWS Marketplace.",
      "E. The ML engineer must deploy the model to a SageMaker endpoint."
    ],
    "answer": "BC"
  },
  {
    "question": "A company has a large, unstructured dataset. The dataset includes many duplicate records across several key attributes.\nWhich solution on AWS will detect duplicates in the dataset with the LEAST code development?",
    "options": [
      "A. Use Amazon Mechanical Turk jobs to detect duplicates.",
      "B. Use Amazon QuickSight ML Insights to build a custom deduplication model.",
      "C. Use Amazon SageMaker Data Wrangler to pre-process and detect duplicates.",
      "D. Use the AWS Glue FindMatches transform to detect duplicates."
    ],
    "answer": "D"
  },
  {
    "question": "A company has deployed an XGBoost prediction model in production to predict if a customer is likely to cancel a subscription. The company uses Amazon SageMaker Model Monitor to detect deviations in the F1 score.\nDuring a baseline analysis of model quality, the company recorded a threshold for the F1 score.\nAfter several months of no change, the model's F1 score decreases significantly.\nWhat could be the reason for the reduced F1 score?",
    "options": [
      "A. Concept drift occurred in the underlying customer data that was used for predictions.",
      "B. The model was not sufficiently complex to capture all the patterns in the original baseline data.",
      "C. The original baseline data had a data quality issue of missing values.",
      "D. Incorrect ground truth labels were provided to Model Monitor during the calculation of the baseline."
    ],
    "answer": "A"
  },
  {
    "question": "A company uses Amazon SageMaker Studio to develop an ML model. The company has a single SageMaker Studio domain. An ML engineer needs to implement a solution that provides an automated alert when SageMaker compute costs reach a specific threshold.\nWhich solution will meet these requirements?",
    "options": [
      "A. Add resource tagging by editing the SageMaker user profile in the SageMaker domain. Configure AWS Cost Explorer to send an alert when the threshold is reached.",
      "B. Add resource tagging by editing the SageMaker user profile in the SageMaker domain. Configure AWS Budgets to send an alert when the threshold is reached.",
      "C. Add resource tagging by editing each user's IAM profile. Configure AWS Cost Explorer to send an alert when the threshold is reached.",
      "D. Add resource tagging by editing each user's IAM profile. Configure AWS Budgets to send an alert when the threshold is reached."
    ],
    "answer": "B"
  },
  {
    "question": "A company is using an Amazon Redshift database as its single data source. Some of the data is sensitive.\nA data scientist needs to use some of the sensitive data from the database. An ML engineer must give the data scientist access to the data without transforming the source data and without storing anonymized data in the database.\nWhich solution will meet these requirements with the LEAST implementation effort?",
    "options": [
      "A. Configure dynamic data masking policies to control how sensitive data is shared with the data scientist at query time.",
      "B. Create a materialized view with masking logic on top of the database. Grant the necessary read permissions to the data scientist.",
      "C. Unload the Amazon Redshift data to Amazon S3. Use Amazon Athena to create schema-on-read with masking logic. Share the view with the data scientist.",
      "D. Unload the Amazon Redshift data to Amazon S3. Create an AWS Glue job to anonymize the data. Share the dataset with the data scientist."
    ],
    "answer": "A"
  },
  {
    "question": "A company wants to build a real-time analytics application that uses streaming data from social media. An ML engineer must implement a solution that ingests and transforms 5 GB of data each minute. The solution also must load the data into a data store that supports fast queries for the real-time analytics. Which solution will meet these requirements?",
    "options": [
      "A. Use Amazon EventBridge to ingest the social media data. Use AWS Glue to transform the data. Store the transformed data in Amazon ElastiCache (Memcached).",
      "B. Use Amazon Simple Queue Service (Amazon SQS) to ingest the social media data. Use AWS Lambda to transform the data. Store the transformed data in Amazon S3.",
      "C. Use Amazon Simple Notification Service (Amazon SNS) to ingest the social media data. Use Amazon EMR to transform the data. Store the transformed data in Amazon RDS.",
      "D. Use Amazon Kinesis Data Streams to ingest the social media data. Use Amazon Managed Service for Apache Flink to transform the data. Store the transformed data in Amazon DynamoDB."
    ],
    "answer": "D",
    "explanation": "Amazon Kinesis Data Streams is designed for high-throughput ingestion of streaming data such as social media feeds. Amazon Managed Service for Apache Flink enables real-time transformations on that data. Amazon DynamoDB provides low-latency reads and writes, making it suitable for fast queries in real-time analytics. This combination fully meets the scale and speed requirements."
  },
  {
    "question": "Hotspot Question\nAn ML engineer must choose the appropriate Amazon SageMaker algorithm to solve specific AI problems.\nSelect the correct SageMaker built-in algorithm from the following list for each use case.\nEach algorithm should be selected one time.\n- Random Cut Forest (RCF) algorithm\n- Semantic segmentation algorithm\n- Sequence-to-Sequence (seq2seq) algorithm\n\n1. Summarize the text of a research paper.\n2. Scan every pixel of an image to help self-driving cars identify objects in their path.\n3. Identify abnormal data points in a dataset.",
    "options": [
      "A. 1: Sequence-to-Sequence, 2: Semantic segmentation, 3: Random Cut Forest",
      "B. 1: Semantic segmentation, 2: Random Cut Forest, 3: Sequence-to-Sequence",
      "C. 1: Random Cut Forest, 2: Sequence-to-Sequence, 3: Semantic segmentation"
    ],
    "answer": "A"
  },
  {
    "question": "A company has an application that uses different APIs to generate embeddings for input text. The company needs to implement a solution to automatically rotate the API tokens every 3 months.\nWhich solution will meet this requirement?",
    "options": [
      "A. Store the tokens in AWS Secrets Manager. Create an AWS Lambda function to perform the rotation.",
      "B. Store the tokens in AWS Systems Manager Parameter Store. Create an AWS Lambda function to perform the rotation.",
      "C. Store the tokens in AWS Key Management Service (AWS KMS). Use an AWS managed key to perform the rotation.",
      "D. Store the tokens in AWS Key Management Service (AWS KMS). Use an AWS owned key to perform the rotation."
    ],
    "answer": "A"
  },
  {
    "question": "A company needs to give its ML engineers appropriate access to training data. The ML engineers must access training data from only their own business group. The ML engineers must not be allowed to access training data from other business groups.\nThe company uses a single AWS account and stores all the training data in Amazon S3 buckets.\nAll ML model training occurs in Amazon SageMaker.\nWhich solution will provide the ML engineers with the appropriate access?",
    "options": [
      "A. Enable S3 bucket versioning.",
      "B. Configure S3 Object Lock settings for each user.",
      "C. Add cross-origin resource sharing (CORS) policies to the S3 buckets.",
      "D. Create IAM policies. Attach the policies to IAM users or IAM roles."
    ],
    "answer": "D"
  },
  {
    "question": "A company is planning to create an internal-only chat interface to help employees handle customer queries. Currently, the employees need to refer to a massive knowledge base of internal documents to address customer issues. The new solution must be serverless. Which combination of steps will meet these requirements?",
    "options": [
      "A. Set up Amazon Bedrock with the Anthropic Claude foundation model.",
      "B. Set up Amazon SageMaker JumpStart with the Llama foundation model.",
      "C. Use Amazon EC2 instances with Amazon API Gateway to invoke the model API.",
      "D. Use AWS Lambda functions with Amazon API Gateway to invoke the model API.",
      "E. Use an Amazon S3 bucket to store vector database dumps and embeddings.",
      "F. Use Amazon RDS for MySQL to store vector database dumps and embeddings."
    ],
    "answer": "ADE",
    "explanation": "To build a serverless internal chat interface, you can use Amazon Bedrock with a foundation model like Claude, invoke the model API through AWS Lambda with Amazon API Gateway, and store embeddings in a vector database format using Amazon S3. This avoids server management, ensures scalability, and leverages serverless components end-to-end."
  },
  {
    "question": "A company is planning to use Amazon SageMaker to make classification ratings that are based on images. The company has 6 GB of training data that is stored on an Amazon FSx for NetApp ONTAP system virtual machine (SVM). The SVM is in the same VPC as SageMaker.\nAn ML engineer must make the training data accessible for ML models that are in the SageMaker environment.\nWhich solution will meet these requirements?",
    "options": [
      "A. Mount the FSx for ONTAP file system as a volume to the SageMaker Instance.",
      "B. Create an Amazon S3 bucket. Use Mountpoint for Amazon S3 to link the S3 bucket to the FSx for ONTAP file system.",
      "C. Create a catalog connection from SageMaker Data Wrangler to the FSx for ONTAP file system.",
      "D. Create a direct connection from SageMaker Data Wrangler to the FSx for ONTAP file system."
    ],
    "answer": "A"
  },
  {
    "question": "An ML engineer needs to merge and transform data from two sources to retrain an existing ML model. One data source consists of .csv files that are stored in an Amazon S3 bucket. Each .csv file consists of millions of records. The other data source is an Amazon Aurora DB cluster.\nThe result of the merge process must be written to a second S3 bucket. The ML engineer needs to perform this merge-and-transform task every week.\nWhich solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "A. Create a transient Amazon EMR cluster every week. Use the cluster to run an Apache Spark job to merge and transform the data.",
      "B. Create a weekly AWS Glue job that uses the Apache Spark engine. Use DynamicFrame native operations to merge and transform the data.",
      "C. Create an AWS Lambda function that runs Apache Spark code every week to merge and transform the data. Configure the Lambda function to connect to the initial S3 bucket and the DB cluster.",
      "D. Create an AWS Batch job that runs Apache Spark code on Amazon EC2 instances every week.Configure the Spark code to save the data from the EC2 instances to the second S3 bucket."
    ],
    "answer": "B"
  },
  {
    "question": "A company is developing a new online application to gather information from customers. An ML engineer has developed a new ML model that will determine a score for each customer. The model will use the score to determine which product to display to the customer. The ML engineer needs to minimize response-time latency for the model. How should the ML engineer deploy the application in Amazon SageMaker to meet these requirements?",
    "options": [
      "A. Configure batch transform.",
      "B. Configure a real-time inference endpoint.",
      "C. Configure a serverless inference endpoint.",
      "D. Configure an asynchronous inference endpoint."
    ],
    "answer": "B",
    "explanation": "To minimize response-time latency, the ML model should be deployed to a real-time inference endpoint in Amazon SageMaker. This provides low-latency predictions by keeping the model loaded and ready to handle incoming requests, which is critical for an online application serving customers in real time."
  },
  {
    "question": "An ML engineer is developing a classification model. The ML engineer needs to use custom libraries in processing jobs, training jobs, and pipelines in Amazon SageMaker. Which solution will provide this functionality with the LEAST implementation effort?",
    "options": [
      "A. Manually install the libraries in the SageMaker containers.",
      "B. Build a custom Docker container that includes the required libraries. Host the container in Amazon Elastic Container Registry (Amazon ECR). Use the ECR image in the SageMaker jobs and pipelines.",
      "C. Create a SageMaker notebook instance to host the jobs. Create an AWS Lambda function to install the libraries on the notebook instance when the notebook instance starts. Configure the SageMaker jobs and pipelines to run on the notebook instance.",
      "D. Run code for the libraries externally on Amazon EC2 instances. Store the results in Amazon S3.Import the results into the SageMaker jobs and pipelines."
    ],
    "answer": "B",
    "explanation": "Building a custom Docker container with the required libraries and hosting it in Amazon ECR allows SageMaker jobs, training, and pipelines to consistently use the same environment. This approach minimizes manual setup, ensures portability, and provides the least ongoing implementation effort compared to repeatedly installing or managing libraries separately."
  },
  {
    "question": "A company has a large collection of chat recordings from customer interactions after a product release. An ML engineer needs to create an ML model to analyze the chat data. The ML engineer needs to determine the success of the product by reviewing customer sentiments about the product.\nWhich action should the ML engineer take to complete the evaluation in the LEAST amount of time?",
    "options": [
      "A. Use Amazon Rekognition to analyze sentiments of the chat conversations.",
      "B. Train a Naive Bayes classifier to analyze sentiments of the chat conversations.",
      "C. Use Amazon Comprehend to analyze sentiments of the chat conversations.",
      "D. Use random forests to classify sentiments of the chat conversations."
    ],
    "answer": "C"
  },
  {
    "question": "Case Study\nA company is building a web-based AI application by using Amazon SageMaker. The application will provide the following capabilities and features: ML experimentation, training, a central model registry, model deployment, and model monitoring.\nThe application must ensure secure and isolated use of training data during the ML lifecycle. The training data is stored in Amazon S3.\nThe company needs to run an on-demand workflow to monitor bias drift for models that are deployed to real-time endpoints from the application.\nWhich action will meet this requirement?",
    "options": [
      "A. Configure the application to invoke an AWS Lambda function that runs a SageMaker Clarify job.",
      "B. Invoke an AWS Lambda function to pull the sagemaker-model-monitor-analyzer built-in SageMaker image.",
      "C. Use AWS Glue Data Quality to monitor bias.",
      "D. Use SageMaker notebooks to compare the bias."
    ],
    "answer": "A"
  },
  {
    "question": "A company is training a large language model (LLM) by using on-premises infrastructure. A live conversational engine uses the LLM to help customers find real-time insights in credit card data.\nAn ML engineer must implement a solution to train and deploy the LLM on Amazon SageMaker.\nWhich solution will meet these requirements?",
    "options": [
      "A. Use SageMaker Training Compiler to train the LLM. Deploy the LLM by using SageMaker real-time inference.",
      "B. Use SageMaker with deep learning containers for large model inference to train the LLM. Deploy the LLM by using SageMaker real-time inference.",
      "C. Use SageMaker Notebook Jobs to train the LLM. Deploy the LLM by using SageMaker Asynchronous Inference.",
      "D. Use SageMaker Studio to train the LLM. Deploy the LLM by using SageMaker batch transform."
    ],
    "answer": "A",
    "explanation": "SageMaker Training Compiler accelerates training of large models like LLMs by optimizing GPU utilization, making it suitable for efficient large-scale training. For deployment of a live conversational engine that requires real-time responses, the correct choice is a SageMaker real-time inference endpoint. This combination meets both training and deployment requirements effectively."
  },
  {
    "question": "A data scientist is evaluating different binary classification models. A false positive result is 5 times more expensive (from a business perspective) than a false negative result.\nThe models should be evaluated based on the following criteria:\n1) Must have a recall rate of at least 80%\n2) Must have a false positive rate of 10% or less\n3) Must minimize business costs\nAfter creating each binary classification model, the data scientist generates the corresponding confusion matrix.\nWhich confusion matrix represents the model that satisfies the requirements?",
    "options": [
      "A. TN = 91, FP = 9, FN = 22, TP = 78",
      "B. TN = 99, FP = 1, FN = 21, TP = 79",
      "C. TN = 96, FP = 4, FN = 10, TP = 90",
      "D. TN = 98, FP = 2, FN = 18, TP = 82"
    ],
    "answer": "D",
    "explanation": "Recall = TP / (TP + FN). False Positive Rate (FPR) = FP / (FP + TN). Cost = 5 * FP + FN.\nModel D:\nRecall = 82 / (82 + 18) = 0.82 (82% >= 80%)\nFPR = 2 / (2 + 98) = 0.02 (2% <= 10%)\nCost = 5 * 2 + 18 = 28 (Lowest cost compared to others)"
  },
  {
    "question": "A company has historical data that shows whether customers needed long-term support from company staff. The company needs to develop an ML model to predict whether new customers will require long-term support.\nWhich modeling approach should the company use to meet this requirement?",
    "options": [
      "A. Anomaly detection",
      "B. Linear regression",
      "C. Logistic regression",
      "D. Semantic segmentation"
    ],
    "answer": "C"
  },
  {
    "question": "A credit card company has a fraud detection model in production on an Amazon SageMaker endpoint. The company develops a new version of the model. The company needs to assess the new model's performance by using live data and without affecting production end users.\nWhich solution will meet these requirements?",
    "options": [
      "A. Set up SageMaker Debugger and create a custom rule.",
      "B. Set up blue/green deployments with all-at-once traffic shifting.",
      "C. Set up blue/green deployments with canary traffic shifting.",
      "D. Set up shadow testing with a shadow variant of the new model."
    ],
    "answer": "D"
  },
  {
    "question": "A company is planning to use Amazon Redshift ML in its primary AWS account. The source data is in an Amazon S3 bucket in a secondary account.\nAn ML engineer needs to set up an ML pipeline in the primary account to access the S3 bucket in the secondary account. The solution must not require public IPv4 addresses.\nWhich solution will meet these requirements?",
    "options": [
      "A. Provision a Redshift cluster and Amazon SageMaker Studio in a VPC with no public access enabled in the primary account. Create a VPC peering connection between the accounts. Update the VPC route tables to remove the route to 0.0.0.0/0.",
      "B. Provision a Redshift cluster and Amazon SageMaker Studio in a VPC with no public access enabled in the primary account. Create an AWS Direct Connect connection and a transit gateway. Associate the VPCs from both accounts with the transit gateway. Update the VPC route tables to remove the route to 0.0.0.0/0.",
      "C. Provision a Redshift cluster and Amazon SageMaker Studio in a VPC in the primary account. Create an AWS Site-to-Site VPN connection with two encrypted IPsec tunnels between the accounts. Set up interface VPC endpoints for Amazon S3.",
      "D. Provision a Redshift cluster and Amazon SageMaker Studio in a VPC in the primary account.Create an S3 gateway endpoint. Update the S3 bucket policy to allow IAM principals from the primary account. Set up interface VPC endpoints for SageMaker and Amazon Redshift."
    ],
    "answer": "D"
  },
  {
    "question": "An ML engineer is training a simple neural network model. The ML engineer tracks the performance of the model over time on a validation dataset. The model's performance improves substantially at first and then degrades after a specific number of epochs.\nWhich solutions will mitigate this problem? (Choose two.)",
    "options": [
      "A. Enable early stopping on the model.",
      "B. Increase dropout in the layers.",
      "C. Increase the number of layers.",
      "D. Increase the number of neurons.",
      "E. Investigate and reduce the sources of model bias."
    ],
    "answer": "AB"
  },
  {
    "question": "An ML engineer needs to use data with Amazon SageMaker Canvas to train an ML model. The data is stored in Amazon S3 and is complex in structure. The ML engineer must use a file format that minimizes processing time for the data.\nWhich file format will meet these requirements?",
    "options": [
      "A. CSV files compressed with Snappy",
      "B. JSON objects in JSONL format",
      "C. JSON files compressed with gzip",
      "D. Apache Parquet files"
    ],
    "answer": "D"
  },
  {
    "question": "An ML engineer has an Amazon Comprehend custom model in Account A in the us-east-1 Region. The ML engineer needs to copy the model to Account B in the same Region.\nWhich solution will meet this requirement with the LEAST development effort?",
    "options": [
      "A. Use Amazon S3 to make a copy of the model. Transfer the copy to Account B.",
      "B. Create a resource-based IAM policy. Use the Amazon Comprehend ImportModel API operation to copy the model to Account B.",
      "C. Use AWS DataSync to replicate the model from Account A to Account B.",
      "D. Create an AWS Site-to-Site VPN connection between Account A and Account B to transfer the model."
    ],
    "answer": "B"
  },
  {
    "question": "A machine learning engineer is preparing a data frame for a supervised learning task with the Amazon SageMaker Linear Learner algorithm. The ML engineer notices the target label classes are highly imbalanced and multiple feature columns contain missing values. The proportion of missing values across the entire data frame is less than 5%.\nWhat should the ML engineer do to minimize bias due to missing values?",
    "options": [
      "A. Replace each missing value by the mean or median across non-missing values in same row.",
      "B. Delete observations that contain missing values because these represent less than 5% of the data.",
      "C. Replace each missing value by the mean or median across non-missing values in the same column.",
      "D. For each feature, approximate the missing values using supervised learning based on other features."
    ],
    "answer": "D",
    "explanation": "Use supervised learning to predict missing values based on the values of other features. Different supervised learning approaches might have different performances, but any properly implemented supervised learning approach should provide the same or better approximation than mean or median approximation, as proposed in responses A and C. Supervised learning applied to the imputation of missing values is an active field of research."
  },
  {
    "question": "A company needs an AWS solution that will automatically create versions of ML models as the models are created.\nWhich solution will meet this requirement?",
    "options": [
      "A. Amazon Elastic Container Registry (Amazon ECR)",
      "B. Model packages from Amazon SageMaker Marketplace",
      "C. Amazon SageMaker ML Lineage Tracking",
      "D. Amazon SageMaker Model Registry"
    ],
    "answer": "D"
  },
  {
    "question": "A company uses 10 Reserved Instances of accelerated instance types to serve the current version of an ML model. An ML engineer needs to deploy a new version of the model to an Amazon SageMaker real-time inference endpoint.\nThe solution must use the original 10 instances to serve both versions of the model. The solution also must include one additional Reserved Instance that is available to use in the deployment process. The transition between versions must occur with no downtime or service interruptions.\nWhich solution will meet these requirements?",
    "options": [
      "A. Configure a blue/green deployment with all-at-once traffic shifting.",
      "B. Configure a blue/green deployment with canary traffic shifting and a size of 10%.",
      "C. Configure a shadow test with a traffic sampling percentage of 10%.",
      "D. Configure a rolling deployment with a rolling batch size of 1."
    ],
    "answer": "D"
  },
  {
    "question": "Hotspot Question\nAn ML engineer is building a generative AI application on Amazon Bedrock by using large language models (LLMs).\nSelect the correct generative AI term from the following list for each description.\n- Text representation of basic units of data processed by LLMs\n- High-dimensional vectors that contain the semantic meaning of text\n- Enrichment of information from additional data sources to improve a generated response",
    "options": [
      "A. Token, Embedding, Retrieval Augmented Generation (RAG)",
      "B. Embedding, Token, Temperature",
      "C. Temperature, RAG, Token",
      "D. RAG, Embedding, Token"
    ],
    "answer": "A"
  },
  {
    "question": "A company is using Amazon SageMaker to create ML models. The company's data scientists need fine-grained control of the ML workflows that they orchestrate. The data scientists also need the ability to visualize SageMaker jobs and workflows as a directed acyclic graph (DAG). The data scientists must keep a running history of model discovery experiments and must establish model governance for auditing and compliance verifications.\nWhich solution will meet these requirements?",
    "options": [
      "A. Use AWS CodePipeline and its integration with SageMaker Studio to manage the entire ML workflows. Use SageMaker ML Lineage Tracking for the running history of experiments and for auditing and compliance verifications.",
      "B. Use AWS CodePipeline and its integration with SageMaker Experiments to manage the entire ML workflows. Use SageMaker Experiments for the running history of experiments and for auditing and compliance verifications.",
      "C. Use SageMaker Pipelines and its integration with SageMaker Studio to manage the entire ML workflows. Use SageMaker ML Lineage Tracking for the running history of experiments and for auditing and compliance verifications.",
      "D. Use SageMaker Pipelines and its integration with SageMaker Experiments to manage the entire ML workflows. Use SageMaker Experiments for the running history of experiments and for auditing and compliance verifications."
    ],
    "answer": "C"
  },
  {
    "question": "A company runs an ML model on Amazon SageMaker. The company uses an automatic process that makes API calls to create training jobs for the model. The company has new compliance rules that prohibit the collection of aggregated metadata from training jobs. Which solution will prevent SageMaker from collecting metadata from the training jobs?",
    "options": [
      "A. Opt out of metadata tracking for any training job that is submitted.",
      "B. Ensure that training jobs are running in a private subnet in a custom VPC.",
      "C. Encrypt the training data with an AWS Key Management Service (AWS KMS) customer managed key.",
      "D. Reconfigure the training jobs to use only AWS Nitro instances."
    ],
    "answer": "A",
    "explanation": "Amazon SageMaker automatically collects training job metadata, but you can opt out of metadata tracking when submitting a training job. This disables collection of aggregated metadata, ensuring compliance with rules that prohibit metadata collection."
  },
  {
    "question": "A company has deployed an ML model that detects fraudulent credit card transactions in real time in a banking application. The model uses Amazon SageMaker Asynchronous Inference. Consumers are reporting delays in receiving the inference results.\nAn ML engineer needs to implement a solution to improve the inference performance. The solution also must provide a notification when a deviation in model quality occurs.\nWhich solution will meet these requirements?",
    "options": [
      "A. Use SageMaker real-time inference for inference. Use SageMaker Model Monitor for notifications about model quality.",
      "B. Use SageMaker batch transform for inference. Use SageMaker Model Monitor for notifications about model quality.",
      "C. Use SageMaker Serverless Inference for inference. Use SageMaker Inference Recommender for notifications about model quality.",
      "D. Keep using SageMaker Asynchronous Inference for inference. Use SageMaker Inference Recommender for notifications about model quality."
    ],
    "answer": "A"
  },
  {
    "question": "A company has developed a new ML model. The company requires online model validation on 10% of the traffic before the company fully releases the model in production. The company uses an Amazon SageMaker endpoint behind an Application Load Balancer (ALB) to serve the model.\nWhich solution will set up the required online validation with the LEAST operational overhead?",
    "options": [
      "A. Use production variants to add the new model to the existing SageMaker endpoint. Set the variant weight to 0.1 for the new model. Monitor the number of invocations by using Amazon CloudWatch.",
      "B. Use production variants to add the new model to the existing SageMaker endpoint. Set the variant weight to 1 for the new model. Monitor the number of invocations by using Amazon CloudWatch.",
      "C. Create a new SageMaker endpoint. Use production variants to add the new model to the new endpoint. Monitor the number of invocations by using Amazon CloudWatch.",
      "D. Configure the ALB to route 10% of the traffic to the new model at the existing SageMaker endpoint. Monitor the number of invocations by using AWS CloudTrail."
    ],
    "answer": "A"
  },
  {
    "question": "Case Study\nAn ML engineer is developing a fraud detection model on AWS. The training dataset includes transaction logs, customer profiles, and tables from an on-premises MySQL database. The transaction logs and customer profiles are stored in Amazon S3.\nThe dataset has a class imbalance that affects the learning of the model's algorithm. Additionally, many of the features have interdependencies. The algorithm is not capturing all the desired underlying patterns in the data.\nBefore the ML engineer trains the model, the ML engineer must resolve the issue of the imbalanced data.\nWhich solution will meet this requirement with the LEAST operational effort?",
    "options": [
      "A. Use Amazon Athena to identify patterns that contribute to the imbalance. Adjust the dataset accordingly.",
      "B. Use Amazon SageMaker Studio Classic built-in algorithms to process the imbalanced dataset.",
      "C. Use AWS Glue DataBrew built-in features to oversample the minority class.",
      "D. Use the Amazon SageMaker Data Wrangler balance data operation to oversample the minority class."
    ],
    "answer": "D"
  },
  {
    "question": "A company needs to develop an ML model. The model must identify an item in an image and must provide the location of the item.\nWhich Amazon SageMaker algorithm will meet these requirements?",
    "options": [
      "A. Image classification",
      "B. XGBoost",
      "C. Object detection",
      "D. K-nearest neighbors (k-NN)"
    ],
    "answer": "C"
  },
  {
    "question": "A company is running ML models on premises by using custom Python scripts and proprietary datasets. The company is using PyTorch. The model building requires unique domain knowledge.\nThe company needs to move the models to AWS.\nWhich solution will meet these requirements with the LEAST effort?",
    "options": [
      "A. Use SageMaker built-in algorithms to train the proprietary datasets.",
      "B. Use SageMaker script mode and premade images for ML frameworks.",
      "C. Build a container on AWS that includes custom packages and a choice of ML frameworks.",
      "D. Purchase similar production models through AWS Marketplace."
    ],
    "answer": "B"
  },
  {
    "question": "A machine learning team has several large CSV datasets in Amazon S3. Historically, models built with the Amazon SageMaker Linear Learner algorithm have taken hours to train on similar-sized datasets. The team's leaders need to accelerate the training process.\nWhat can a machine learning specialist do to address this concern?",
    "options": [
      "A. Use Amazon SageMaker Pipe mode.",
      "B. Use Amazon Machine Learning to train the models.",
      "C. Use Amazon Kinesis to stream the data to Amazon SageMaker.",
      "D. Use AWS Glue to transform the CSV dataset to the JSON format."
    ],
    "answer": "A",
    "explanation": "Amazon SageMaker Pipe mode streams the data directly to the container, which improves the performance of training jobs. In Pipe mode, your training job streams data directly from Amazon S3. Streaming can provide faster start times for training jobs and better throughput. With Pipe mode, you also reduce the size of the Amazon EBS volumes for your training instances."
  },
  {
    "question": "A company is using an Amazon S3 bucket to collect data that will be used for ML workflows. The company needs to use AWS Glue DataBrew to clean and normalize the data. Which solution will meet these requirements?",
    "options": [
      "A. Create a DataBrew dataset by using the S3 path. Clean and normalize the data by using a DataBrew profile job.",
      "B. Create a DataBrew dataset by using the S3 path. Clean and normalize the data by using a DataBrew recipe job.",
      "C. Create a DataBrew dataset by using a Java Database Connectivity (JDBC) driver to connect to the S3 bucket. Clean and normalize the data by using a DataBrew profile job.",
      "D. Create a DataBrew dataset by using a Java Database Connectivity (JDBC) driver to connect to the S3 bucket. Clean and normalize the data by using a DataBrew recipe job."
    ],
    "answer": "B",
    "explanation": "The correct solution is to create a DataBrew dataset using the S3 path and then clean and normalize the data with a DataBrew recipe job. Recipes define and apply transformations to the data, while profile jobs are used only for data analysis and profiling, not cleaning."
  },
  {
    "question": "Hotspot Question\nA company stores historical data in .csv files in Amazon S3. Only some of the rows and columns in the .csv files are populated. The columns are not labeled. An ML engineer needs to prepare and store the data so that the company can use the data to train ML models.\nSelect and order the correct steps from the following list to perform this task. Each step should be selected one time or not at all. (Select and order three.)\n- Create an Amazon SageMaker batch transform job for data cleaning and feature engineering.\n- Store the resulting data back in Amazon S3.\n- Use Amazon Athena to infer the schemas and available columns.\n- Use AWS Glue crawlers to infer the schemas and available columns.\n- Use AWS Glue DataBrew for data cleaning and feature engineering.",
    "options": [
      "A. 1: Use AWS Glue crawlers..., 2: Use AWS Glue DataBrew..., 3: Store the resulting data...",
      "B. 1: Use Amazon Athena..., 2: Create an Amazon SageMaker batch transform..., 3: Store the resulting data...",
      "C. 1: Use AWS Glue crawlers..., 2: Create an Amazon SageMaker batch transform..., 3: Store the resulting data..."
    ],
    "answer": "A"
  },
  {
    "question": "A company has trained and deployed an ML model by using Amazon SageMaker. The company needs to implement a solution to record and monitor all the API call events for the SageMaker endpoint. The solution also must provide a notification when the number of API call events breaches a threshold.\nWhich solution will meet these requirements?",
    "options": [
      "A. Use SageMaker Debugger to track the inferences and to report metrics. Create a custom rule to provide a notification when the threshold is breached.",
      "B. Use SageMaker Debugger to track the inferences and to report metrics. Use the tensor_variance built-in rule to provide a notification when the threshold is breached.",
      "C. Log all the endpoint invocation API events by using AWS CloudTrail. Use an Amazon CloudWatch dashboard for monitoring. Set up a CloudWatch alarm to provide notification when the threshold is breached.",
      "D. Add the Invocations metric to an Amazon CloudWatch dashboard for monitoring. Set up a CloudWatch alarm to provide notification when the threshold is breached."
    ],
    "answer": "C"
  },
  {
    "question": "An ML engineer needs to deploy a trained model that is based on a genetic algorithm. The algorithm solves a complex problem and can take several minutes to generate predictions. When the model is deployed, the model needs to access large amounts of data to process requests. The requests can involve as much as 100 MB of data.\nWhich deployment solution will meet these requirements with the LEAST operational overhead?",
    "options": [
      "A. Deploy the model to Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer.",
      "B. Deploy the model to an Amazon SageMaker real-time inference endpoint.",
      "C. Deploy the model to an Amazon SageMaker Asynchronous Inference endpoint.",
      "D. Package the model as a container. Deploy the model to Amazon Elastic Container Service (Amazon ECS) on Amazon EC2 instances."
    ],
    "answer": "C",
    "explanation": "SageMaker Asynchronous Inference is designed for models with long processing times and large payloads. It can handle input data up to 1 GB and avoids holding open connections during long inference runs, reducing operational overhead compared to managing EC2 or ECS infrastructure."
  },
  {
    "question": "A company is creating an application that will recommend products for customers to purchase. The application will make API calls to Amazon Q Business. The company must ensure that responses from Amazon Q Business do not include the name of the company's main competitor.\nWhich solution will meet this requirement?",
    "options": [
      "A. Configure the competitor's name as a blocked phrase in Amazon Q Business.",
      "B. Configure an Amazon Q Business retriever to exclude the competitor's name.",
      "C. Configure an Amazon Kendra retriever for Amazon Q Business to build indexes that exclude the competitor's name.",
      "D. Configure document attribute boosting in Amazon Q Business to deprioritize the competitor's name."
    ],
    "answer": "A"
  },
  {
    "question": "A company has trained an ML model in Amazon SageMaker. The company needs to host the model to provide inferences in a production environment. The model must be highly available and must respond with minimum latency. The size of each request will be between 1 KB and 3 MB. The model will receive unpredictable bursts of requests during the day. The inferences must adapt proportionally to the changes in demand.\nHow should the company deploy the model into production to meet these requirements?",
    "options": [
      "A. Create a SageMaker real-time inference endpoint. Configure auto scaling. Configure the endpoint to present the existing model.",
      "B. Deploy the model on an Amazon Elastic Container Service (Amazon ECS) cluster. Use ECS scheduled scaling that is based on the CPU of the ECS cluster.",
      "C. Install SageMaker Operator on an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Deploy the model in Amazon EKS. Set horizontal pod auto scaling to scale replicas based on the memory metric.",
      "D. Use Spot Instances with a Spot Fleet behind an Application Load Balancer (ALB) for inferences.Use the ALBRequestCountPerTarget metric as the metric for auto scaling."
    ],
    "answer": "A"
  },
  {
    "question": "Case Study\nAn ML engineer is developing a fraud detection model on AWS. The training dataset includes transaction logs, customer profiles, and tables from an on-premises MySQL database. The transaction logs and customer profiles are stored in Amazon S3. The dataset has a class imbalance that affects the learning of the model's algorithm. Additionally, many of the features have interdependencies. The algorithm is not capturing all the desired underlying patterns in the data.\nWhich AWS service or feature can aggregate the data from the various data sources?",
    "options": [
      "A. Amazon EMR Spark jobs",
      "B. Amazon Kinesis Data Streams",
      "C. Amazon DynamoDB",
      "D. AWS Lake Formation"
    ],
    "answer": "D"
  },
  {
    "question": "A company is exploring generative AI and wants to add a new product feature. An ML engineer is making API calls from existing Amazon EC2 instances to Amazon Bedrock. The EC2 instances are in a private subnet and must remain private during the implementation. The EC2 instances have an assigned security group that allows access to all IP addresses in the private subnet.\nWhat should the ML engineer do to establish a connection between the EC2 instances and Amazon Bedrock?",
    "options": [
      "A. Modify the security group to allow inbound and outbound traffic to and from Amazon Bedrock.",
      "B. Use AWS PrivateLink to access Amazon Bedrock through an interface VPC endpoint.",
      "C. Configure Amazon Bedrock to use the private subnet where the EC2 instances are deployed.",
      "D. Link the existing VPC to Amazon Bedrock by using an AWS Direct Connect connection."
    ],
    "answer": "B",
    "explanation": "Since the EC2 instances are in a private subnet and must not have public internet access, the correct solution is to use AWS PrivateLink with an interface VPC endpoint for Amazon Bedrock. This allows private connectivity from the VPC to the Bedrock service without exposing traffic to the public internet."
  },
  {
    "question": "An ML engineer wants an Amazon SageMaker notebook to automatically stop running after 1 hour of idle time. How can the ML engineer accomplish this goal?",
    "options": [
      "A. Create a lifecycle configuration in SageMaker. Copy the auto-stop-idle script from GitHub to the Start Notebook section.",
      "B. Create a lifecycle configuration in SageMaker. Copy the auto-stop-idle script from GitHub to the Create Notebook section.",
      "C. Track the notebook's CPU metric by using Amazon CloudWatch Logs. Invoke an AWS Lambda function from CloudWatch Logs to shut down the notebook instance if CPU utilization becomes zero.",
      "D. Track the notebook's memory metric by using Amazon CloudWatch Logs. Invoke an AWS Lambda function from CloudWatch Logs to shut down the notebook instance if memory utilization becomes zero."
    ],
    "answer": "A",
    "explanation": "The correct approach is to use a SageMaker lifecycle configuration and place the auto-stop-idle script in the Start Notebook section. This ensures the notebook runs the monitoring script on startup, which checks for idle time and automatically stops the notebook after the defined threshold (1 hour in this case)."
  },
  {
    "question": "A company has used Amazon SageMaker to deploy a predictive ML model in production. The company is using SageMaker Model Monitor on the model. After a model update, an ML engineer notices data quality issues in the Model Monitor checks.\nWhat should the ML engineer do to mitigate the data quality issues that Model Monitor has identified?",
    "options": [
      "A. Adjust the model's parameters and hyperparameters.",
      "B. Initiate a manual Model Monitor job that uses the most recent production data.",
      "C. Create a new baseline from the latest dataset. Update Model Monitor to use the new baseline for evaluations.",
      "D. Include additional data in the existing training set for the model. Retrain and redeploy the model."
    ],
    "answer": "C"
  },
  {
    "question": "An ML engineer is using Amazon SageMaker to train a deep learning model that requires distributed training. After some training attempts, the ML engineer observes that the instances are not performing as expected. The ML engineer identifies communication overhead between the training instances.\nWhat should the ML engineer do to MINIMIZE the communication overhead between the instances?",
    "options": [
      "A. Place the instances in the same VPC subnet. Store the data in a different AWS Region from where the instances are deployed.",
      "B. Place the instances in the same VPC subnet but in different Availability Zones. Store the data in a different AWS Region from where the instances are deployed.",
      "C. Place the instances in the same VPC subnet. Store the data in the same AWS Region and Availability Zone where the instances are deployed.",
      "D. Place the instances in the same VPC subnet. Store the data in the same AWS Region but in a different Availability Zone from where the instances are deployed."
    ],
    "answer": "C"
  },
  {
    "question": "A company is using Amazon SageMaker to develop ML models. The company stores sensitive training data in an Amazon S3 bucket. The model training must have network isolation from the internet.\nWhich solution will meet this requirement?",
    "options": [
      "A. Run the SageMaker training jobs in private subnets. Create a NAT gateway. Route traffic for training through the NAT gateway.",
      "B. Run the SageMaker training jobs in private subnets. Create an S3 gateway VPC endpoint. Route traffic for training through the S3 gateway VPC endpoint.",
      "C. Run the SageMaker training jobs in public subnets that have an attached security group. In the security group, use inbound rules to limit traffic from the internet. Encrypt SageMaker instance storage by using server-side encryption with AWS KMS keys (SSE-KMS).",
      "D. Encrypt traffic to Amazon S3 by using a bucket policy that includes a value of True for the aws:SecureTransport condition key. Use default at-rest encryption for Amazon S3. Encrypt SageMaker instance storage by using server-side encryption with AWS KMS keys (SSE KMS)."
    ],
    "answer": "B"
  }
]